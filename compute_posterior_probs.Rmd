---
title: "Computation of posterior probability errors for different approximations of the Matérn covariance function"
author: "David Bolin, Vaibhav Mehandiratta, and Alexandre B. Simas"
date: "Created: 2024-05-20. Last modified: `r Sys.Date()`."
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Computation of posterior probability errors for different approximations of the Matérn covariance function}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
references:
- id: BMS2024
  title: "Linear cost and exponentially convergent approximation of Gaussian Matérn stochastic processes"
  author:
  - family: Bolin
    given: David
  - family: Mehandiratta
    given: Vaibhav
  - family: Simas
    given: Alexandre B.
  container-title: Preprint
- id: DBFG
  title: "Hierarchical nearest-neighbor Gaussian process models for large geostatistical datasets"
  author:
  - family: Datta
    given: A.
  - family: Benerjee
    given: S. 
  - family: Finley
    given: A. O.
  - family: Gelfand
    given: A. E.
  container-title: J. Amer. Statist. Assoc.
  type: article
  issue: 111
  pages: 800-812
  issued:
    year: 2016
- id: KS
  title: "Approximate state-space Gaussian processes via spectral transformation"
  author:
  - family: Karvonen
    given: T.
  - family: Sarkkä
    given: S. 
  container-title: 2016 IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP)
  type: article
  pages: 1-6
  issued:
    year: 2016
- id: RR
  title: "Random features for large-scale kernel machines"
  author:
  - family: Rahimi
    given: A.
  - family: Recht
    given: B.
  container-title:  Advances in neural information processing systems
  type: article
  issue: 20
  issued:
    year: 2007
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
set.seed(1)
```

## Introduction

This vignette contains posterior probability errors.

Our goal is to generate the data as 
$$Y_i = u(t_i) + \varepsilon_i,\quad i=1,\ldots,N,$$
where $u(\cdot)$ is a stationary Gaussian process with a Mat\'ern covariance function, $t_1,\ldots,t_N$ are locations on a bounded interval on the real line, and $\varepsilon_1,\ldots,\varepsilon_N$ are measurement errors, such that $\varepsilon_1,\ldots,\varepsilon_N$ are i.i.d., independent of the field $u(\cdot)$ and follow standard normal distribution. Then, we will compute the posterior mean using the exact Matérn covariance function and we will compare to the posterior means obtained by using the different approximations of the covariance function.

We will then compute approximations of the probability
$$P(u(t_i)  \geq y_i: i\in I | Y),$$
where $t_i$ are some fixed locations and $I$ is an index set.

## Computing the errors

Let us load all the auxiliary functions:

```{r, message=FALSE}
source("aux_functions/aux_functions_cov.R")
source("aux_functions/probability_computations.R")
```

We will now consider the basic setup:

```{r}
N <- 250
N_prob <- 100
obs_ind <- sort(sample(1:N, N-N_prob))
prob_ind <- which(!(1:N %in% obs_ind))
L <- 10
range <- 1
sigma <- 1
nu_vec=seq(0.1, 1.5,  by = 0.1)
idx <- (nu_vec + 0.5)%%1 > 1e-10
nu_vec <- nu_vec[idx]
samples = 10
m <- 1:6
# Std. dev. of the measurement error
sigma_e <- 0.01

loc <- seq(0,L, length.out = N)

m_nngp_fun <- function(m, alpha){
              if(alpha<1) {
                mn <- m
            } else if (alpha < 2) {
                mn <- round(m*(ceil(alpha)+1)^1.5)    
            } else {
                mn <- round(m*(ceil(alpha)+1)^1.5)
            }
            return(mn)
} 
m_pca_fun <- function(m, alpha){
      return(pmax(9*round((m+1)*sqrt(m) * ceil(alpha)^(3/2)),2))
}
m_statespace_fun <- function(m, alpha){
    return(m+floor(alpha) + 1)
}
```

We will now compute the probability estimates based on the above calibration, which is taken with respect to prediction timings.

It is important to observe that we are computing the posterior probabilities for nearest neighbor Gaussian models on locations that are not supporting points. That is, we assume that the location in which we want to do prediction is a new location to the model.

Note that it is possible to have unobserved locations that are supporting points for the nearest neighbor Gaussian processes, however in this case, a practitioner would need to know he/she would be interested in studying that location at the time they were building the model, which is an unrealistic assumption.

```{r}
prob_ests <- compute_prob_ests(loc_full = loc, obs_ind = obs_ind, loc_prob_idx = prob_ind, L_statespace = L, range = range, sigma = sigma, nu_vec = nu_vec, sigma_e = sigma_e, m_rat = m, m_nngp_fun = m_nngp_fun, 
m_pca_fun = m_pca_fun, m_fourier_fun = m_fourier_fun, m_statespace_fun = m_statespace_fun, samples = 10)
```